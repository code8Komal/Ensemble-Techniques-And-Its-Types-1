{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59289a5-82da-4598-970b-15aed159cb1f",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c2eb8-b2f1-4c54-b01d-0ed7bdb4ef62",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning involves combining multiple base models to build a stronger and more robust predictive model. Instead of relying on the prediction of a single model, ensemble methods leverage the collective wisdom of multiple models to improve predictive performance, generalization, and robustness.\n",
    "\n",
    "The basic idea behind ensemble methods is to create a diverse set of base models that make different types of errors on the training data. By combining these models in a strategic way, ensemble methods can reduce bias, variance, and overfitting, leading to better overall performance.\n",
    "\n",
    "There are several popular ensemble techniques in machine learning, including:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**:\n",
    "   - Bagging involves training multiple base models (often decision trees) independently on random subsets of the training data (with replacement).\n",
    "   - The final prediction is typically made by averaging or voting the predictions of all base models.\n",
    "   - Bagging helps reduce variance and overfitting by introducing randomness into the training process.\n",
    "\n",
    "2. **Boosting**:\n",
    "   - Boosting builds a sequence of base models, where each subsequent model focuses on correcting the errors of the previous models.\n",
    "   - Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "   - Boosting aims to reduce bias and improve predictive performance by iteratively fitting models to the residuals of the previous models.\n",
    "\n",
    "3. **Random Forest**:\n",
    "   - Random Forest is an ensemble method that combines the concepts of bagging and decision trees.\n",
    "   - It trains a large number of decision trees on random subsets of the data and averages their predictions to make the final prediction.\n",
    "   - Random Forest improves upon the high variance of individual decision trees while maintaining their interpretability and scalability.\n",
    "\n",
    "4. **Stacking**:\n",
    "   - Stacking (or stacked generalization) combines the predictions of multiple base models using a meta-model (or blender).\n",
    "   - The base models are trained independently, and their predictions serve as input features for the meta-model.\n",
    "   - Stacking can capture complex interactions between base models and often leads to better performance than individual models.\n",
    "\n",
    "Ensemble techniques are widely used in various machine learning tasks and are known for their effectiveness in improving predictive performance and robustness. They are particularly useful when dealing with complex datasets, noisy data, or when individual models perform poorly on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ff0b7-b020-480c-90e5-6ed35b7c0f77",
   "metadata": {},
   "source": [
    "## Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec8d34-09d9-40c9-9971-250734c04d6e",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, primarily because they offer numerous advantages over single models. Here are some key reasons why ensemble techniques are widely used:\n",
    "\n",
    "1. **Improved Predictive Performance**:\n",
    "   - Ensemble methods often result in higher predictive accuracy compared to individual base models. By combining the predictions of multiple models, ensemble techniques can capture a wider range of patterns and relationships in the data, leading to more accurate predictions.\n",
    "\n",
    "2. **Reduction of Overfitting**:\n",
    "   - Ensemble methods can help mitigate overfitting, especially in complex models with high variance. By combining multiple base models that make different types of errors, ensemble techniques can smooth out the noise and reduce the risk of overfitting to the training data.\n",
    "\n",
    "3. **Enhanced Robustness**:\n",
    "   - Ensemble techniques tend to be more robust to outliers, noise, and data variability. Since ensemble methods aggregate the predictions of multiple models, they are less sensitive to individual model errors or anomalies in the data.\n",
    "\n",
    "4. **Better Generalization**:\n",
    "   - Ensemble methods often generalize well to unseen data. By leveraging the wisdom of multiple models, ensemble techniques can capture more robust and reliable patterns in the data, leading to better generalization performance on new, unseen instances.\n",
    "\n",
    "5. **Capturing Complex Relationships**:\n",
    "   - Ensemble methods are capable of capturing complex relationships and interactions in the data that may be difficult for individual models to learn. By combining diverse models trained on different subsets of the data or using different algorithms, ensemble techniques can effectively model intricate patterns in the data.\n",
    "\n",
    "6. **Flexibility and Adaptability**:\n",
    "   - Ensemble techniques are flexible and can be applied to a wide range of machine learning tasks and algorithms. They can be easily integrated with different base models and can adapt to various problem domains and data types.\n",
    "\n",
    "7. **Interpretability and Explainability**:\n",
    "   - Ensemble methods can improve the interpretability and explainability of machine learning models. By combining simpler base models, such as decision trees or linear models, ensemble techniques can produce more interpretable results while still maintaining high predictive performance.\n",
    "\n",
    "Overall, ensemble techniques are popular in machine learning because they offer a powerful and versatile approach to building predictive models that are more accurate, robust, and generalizable compared to individual models. They are widely used across various domains and applications where high-performance predictive modeling is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf7cc1-8b71-44bc-be4c-1c749881fb58",
   "metadata": {},
   "source": [
    "## Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cf069-72ca-45ca-aa71-4fb9f9a13485",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the stability and accuracy of predictive models by reducing variance and overfitting. Bagging works by training multiple base models independently on different subsets of the training data and then aggregating their predictions to make the final prediction.\n",
    "\n",
    "The key steps involved in bagging are as follows:\n",
    "\n",
    "1. **Bootstrap Sampling**:\n",
    "   - Bagging begins by creating multiple bootstrap samples from the original training data.\n",
    "   - Bootstrap sampling involves randomly sampling with replacement from the training data to create multiple subsets of the same size as the original dataset.\n",
    "   - Each bootstrap sample is used as a training set for a base model.\n",
    "\n",
    "2. **Base Model Training**:\n",
    "   - After generating bootstrap samples, a base model (often a decision tree) is trained independently on each bootstrap sample.\n",
    "   - Each base model learns to capture different patterns and relationships present in the training data due to the randomness introduced by bootstrap sampling.\n",
    "\n",
    "3. **Prediction Aggregation**:\n",
    "   - Once all base models are trained, they are used to make predictions on new instances or the test data.\n",
    "   - For classification tasks, the final prediction is typically determined by majority voting among the predictions of all base models.\n",
    "   - For regression tasks, the final prediction is often calculated as the average or median of the predictions made by all base models.\n",
    "\n",
    "The key advantages of bagging include:\n",
    "\n",
    "- **Reduced Variance**: Bagging reduces the variance of the predictions by averaging or voting over multiple base models, which helps mitigate overfitting.\n",
    "- **Improved Stability**: By training base models on different subsets of the data, bagging improves the stability of the ensemble model, making it less sensitive to fluctuations or noise in the training data.\n",
    "- **Better Generalization**: Bagging often leads to better generalization performance on unseen data by capturing a more robust set of patterns and relationships present in the data.\n",
    "\n",
    "Popular algorithms that utilize bagging include Random Forest for decision trees and Bagged Decision Trees. Bagging is a fundamental technique in ensemble learning and is widely used across various machine learning tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d062c-d7b0-4aa9-b377-81e360466c51",
   "metadata": {},
   "source": [
    "## Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f32a6-3d42-40f0-a540-b4b3f580bcff",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique in machine learning that combines multiple weak learners (typically simple models) to create a strong learner with improved predictive performance. Unlike bagging, which trains base models independently and then combines their predictions, boosting builds a sequence of base models iteratively, where each subsequent model focuses on correcting the errors of the previous models.\n",
    "\n",
    "The key steps involved in boosting are as follows:\n",
    "\n",
    "1. **Base Model Training**:\n",
    "   - Boosting starts by training a base model (often a decision tree) on the entire training dataset.\n",
    "   - The initial model is usually a simple model that performs slightly better than random guessing.\n",
    "\n",
    "2. **Sequential Model Building**:\n",
    "   - After training the initial base model, boosting iteratively builds a sequence of additional base models, each focusing on the instances that the previous models struggled to classify correctly.\n",
    "   - Each subsequent model is trained on a modified version of the training data, where the weights of misclassified instances are increased to make them more influential in the training process.\n",
    "   - The goal is to iteratively reduce the errors made by the ensemble by emphasizing the \"hard\" instances that were misclassified in previous iterations.\n",
    "\n",
    "3. **Weighted Voting or Combining Predictions**:\n",
    "   - Once all base models are trained, their predictions are combined using a weighted voting scheme.\n",
    "   - In classification tasks, the final prediction is typically determined by weighted voting, where the weight of each base model's prediction is proportional to its performance on the training data.\n",
    "   - In regression tasks, the final prediction is often calculated as a weighted average of the predictions made by all base models.\n",
    "\n",
    "Boosting algorithms vary in their specific implementations and techniques for adjusting instance weights and constructing subsequent base models. Some popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting).\n",
    "\n",
    "The key advantages of boosting include:\n",
    "\n",
    "- **Improved Predictive Performance**: Boosting often results in higher predictive accuracy compared to individual base models, especially on complex datasets.\n",
    "- **Robustness to Overfitting**: Boosting can reduce overfitting by iteratively focusing on correcting the errors of the previous models, leading to better generalization performance.\n",
    "- **Ability to Capture Complex Relationships**: Boosting can capture complex relationships and interactions in the data by iteratively refining the ensemble model.\n",
    "\n",
    "Boosting is widely used in various machine learning tasks and domains due to its effectiveness in improving predictive performance and robustness. However, it may be more computationally expensive and sensitive to noisy data compared to other ensemble techniques such as bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d60ca-2de9-4ef7-8a85-518fdd3e7325",
   "metadata": {},
   "source": [
    "## Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90af6e-b91d-4f2b-9763-9ea362c69ec3",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, making them popular and widely used in various applications. Some of the key benefits of using ensemble techniques include:\n",
    "\n",
    "1. **Improved Predictive Performance**:\n",
    "   - Ensemble techniques often lead to higher predictive accuracy compared to individual base models. By combining the predictions of multiple models, ensemble methods can capture a wider range of patterns and relationships in the data, leading to more accurate predictions.\n",
    "\n",
    "2. **Reduction of Overfitting**:\n",
    "   - Ensemble methods can help mitigate overfitting, especially in complex models with high variance. By combining multiple base models that make different types of errors, ensemble techniques can smooth out the noise and reduce the risk of overfitting to the training data.\n",
    "\n",
    "3. **Enhanced Robustness**:\n",
    "   - Ensemble techniques tend to be more robust to outliers, noise, and data variability. Since ensemble methods aggregate the predictions of multiple models, they are less sensitive to individual model errors or anomalies in the data.\n",
    "\n",
    "4. **Better Generalization**:\n",
    "   - Ensemble methods often generalize well to unseen data. By leveraging the wisdom of multiple models, ensemble techniques can capture more robust and reliable patterns in the data, leading to better generalization performance on new, unseen instances.\n",
    "\n",
    "5. **Capturing Complex Relationships**:\n",
    "   - Ensemble methods are capable of capturing complex relationships and interactions in the data that may be difficult for individual models to learn. By combining diverse models trained on different subsets of the data or using different algorithms, ensemble techniques can effectively model intricate patterns in the data.\n",
    "\n",
    "6. **Flexibility and Adaptability**:\n",
    "   - Ensemble techniques are flexible and can be applied to a wide range of machine learning tasks and algorithms. They can be easily integrated with different base models and can adapt to various problem domains and data types.\n",
    "\n",
    "7. **Interpretability and Explainability**:\n",
    "   - Ensemble methods can improve the interpretability and explainability of machine learning models. By combining simpler base models, such as decision trees or linear models, ensemble techniques can produce more interpretable results while still maintaining high predictive performance.\n",
    "\n",
    "Overall, ensemble techniques offer a powerful and versatile approach to building predictive models that are more accurate, robust, and generalizable compared to individual models. They are widely used across various domains and applications where high-performance predictive modeling is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8482ca-34dd-4aa4-af62-77bbdfac9bba",
   "metadata": {},
   "source": [
    "## Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd984395-4e91-4f47-a505-4fb128f11f48",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning and often lead to improved predictive performance compared to individual models. However, whether ensemble techniques are always better than individual models depends on various factors, including the characteristics of the dataset, the complexity of the problem, and the specific ensemble method used. Here are some considerations:\n",
    "\n",
    "1. **Dataset Characteristics**:\n",
    "   - Ensemble techniques tend to perform well on large, diverse datasets with complex patterns and relationships. If the dataset is small or simple, individual models may achieve comparable or even better performance without the overhead of ensemble methods.\n",
    "\n",
    "2. **Model Diversity**:\n",
    "   - The effectiveness of ensemble techniques depends on the diversity of the base models. If the base models are too similar or highly correlated, ensemble methods may not provide significant performance gains. Therefore, it's essential to use diverse base models to maximize the benefits of ensemble techniques.\n",
    "\n",
    "3. **Computational Resources**:\n",
    "   - Ensemble techniques often require more computational resources (e.g., memory, CPU time) compared to individual models, especially when training large ensembles or complex algorithms. In scenarios where computational resources are limited, using individual models may be more practical.\n",
    "\n",
    "4. **Interpretability and Complexity**:\n",
    "   - Ensemble techniques may sacrifice interpretability and simplicity in favor of improved predictive performance. In some cases, especially in domains where model interpretability is crucial (e.g., healthcare, finance), using simpler individual models may be preferred over complex ensemble methods.\n",
    "\n",
    "5. **Overfitting and Regularization**:\n",
    "   - Ensemble techniques can help mitigate overfitting by combining multiple models that make different types of errors. However, in some cases, individual models with proper regularization techniques may achieve comparable or better generalization performance without the need for ensembling.\n",
    "\n",
    "6. **Domain-Specific Considerations**:\n",
    "   - The effectiveness of ensemble techniques may vary depending on the specific characteristics of the problem domain. Certain domains or applications may benefit more from ensemble methods, while others may not see significant improvements.\n",
    "\n",
    "In summary, while ensemble techniques are powerful tools for improving predictive performance in many cases, they are not always superior to individual models. The decision to use ensemble methods should be based on careful consideration of the dataset characteristics, computational resources, interpretability requirements, and other domain-specific considerations. It's essential to experiment with different approaches and evaluate the performance of both individual models and ensemble methods to determine the best approach for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6261a4-0474-41ee-a8b7-5e1bc9725a3b",
   "metadata": {},
   "source": [
    "## Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b78f0-ee1d-440f-9ea4-434271470278",
   "metadata": {},
   "source": [
    "The confidence interval (CI) calculated using bootstrap resampling involves estimating the uncertainty or variability of a statistic (such as the mean, median, or standard deviation) by repeatedly resampling from the original dataset. Here's how the process typically works:\n",
    "\n",
    "1. **Resampling**:\n",
    "   - Bootstrap resampling involves randomly sampling with replacement from the original dataset to create multiple bootstrap samples.\n",
    "   - Each bootstrap sample has the same size as the original dataset but may contain duplicate instances due to sampling with replacement.\n",
    "\n",
    "2. **Statistic Calculation**:\n",
    "   - For each bootstrap sample, the statistic of interest (e.g., mean, median, standard deviation) is calculated.\n",
    "   - This statistic represents an estimate of the parameter of interest based on the resampled data.\n",
    "\n",
    "3. **Empirical Distribution**:\n",
    "   - After calculating the statistic for each bootstrap sample, we have a distribution of bootstrap estimates.\n",
    "   - This distribution is referred to as the empirical distribution of the statistic.\n",
    "\n",
    "4. **Confidence Interval Calculation**:\n",
    "   - The confidence interval is then calculated based on the empirical distribution of the statistic.\n",
    "   - The confidence interval provides a range of values that is likely to contain the true parameter value with a certain level of confidence (e.g., 95% confidence interval).\n",
    "   - Common methods for calculating confidence intervals using bootstrap include percentile method, basic bootstrap method, and bias-corrected and accelerated (BCa) bootstrap method.\n",
    "\n",
    "   - **Percentile Method**: The percentile method involves sorting the bootstrap estimates in ascending order and selecting the (α/2)th and (1 - α/2)th percentiles as the lower and upper bounds of the confidence interval, respectively. For example, a 95% confidence interval would use the 2.5th and 97.5th percentiles.\n",
    "   \n",
    "   - **Basic Bootstrap Method**: The basic bootstrap method involves calculating the sample quantiles of the bootstrap estimates, where the lower and upper bounds of the confidence interval are defined as the sample quantiles corresponding to (α/2)th and (1 - α/2)th percentiles, respectively.\n",
    "   \n",
    "   - **BCa Bootstrap Method**: The BCa bootstrap method adjusts the percentile confidence interval for bias and skewness in the bootstrap distribution. It incorporates additional correction terms to improve the accuracy of the confidence interval estimates.\n",
    "\n",
    "By calculating the confidence interval using bootstrap resampling, we can quantify the uncertainty associated with the statistic of interest and make more informed decisions based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36aa1d3-d430-4da9-8ab1-0d121d8dcd14",
   "metadata": {},
   "source": [
    "## Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6eb57f-468c-41fb-9b6f-2b896655a69e",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic or to assess the uncertainty associated with a sample estimate. It involves generating multiple bootstrap samples by sampling with replacement from the original dataset and then using these samples to estimate a parameter, compute confidence intervals, or perform hypothesis testing. Here are the key steps involved in bootstrap:\n",
    "\n",
    "1. **Original Dataset**:\n",
    "   - Start with a dataset containing \\( n \\) observations or samples.\n",
    "\n",
    "2. **Resampling**:\n",
    "   - Randomly sample with replacement from the original dataset to create multiple bootstrap samples.\n",
    "   - Each bootstrap sample has the same size as the original dataset (\\( n \\)) but may contain duplicate instances due to sampling with replacement.\n",
    "\n",
    "3. **Estimation or Statistical Calculation**:\n",
    "   - For each bootstrap sample, calculate the statistic or parameter of interest.\n",
    "   - This statistic could be the mean, median, standard deviation, proportion, or any other measure depending on the objective of the analysis.\n",
    "\n",
    "4. **Analysis of Bootstrap Samples**:\n",
    "   - Analyze the distribution of the bootstrap estimates obtained from the resampled datasets.\n",
    "   - This analysis could involve computing summary statistics (e.g., mean, median, standard deviation) or constructing confidence intervals.\n",
    "\n",
    "5. **Estimate of the Parameter**:\n",
    "   - Use the statistics calculated from the bootstrap samples to estimate the parameter of interest.\n",
    "   - The bootstrap estimate of the parameter is typically the average or median of the bootstrap estimates obtained from the resampled datasets.\n",
    "\n",
    "6. **Assessment of Uncertainty**:\n",
    "   - Assess the uncertainty associated with the parameter estimate by calculating confidence intervals using the bootstrap samples.\n",
    "   - Confidence intervals provide a range of values within which the true parameter value is likely to fall with a specified level of confidence.\n",
    "\n",
    "7. **Inference and Decision Making**:\n",
    "   - Use the estimated parameter and associated uncertainty to make inferences or decisions based on the data.\n",
    "   - For example, conduct hypothesis testing, compare groups, or assess the effect of interventions.\n",
    "\n",
    "By repeatedly resampling from the original dataset, bootstrap allows us to estimate the sampling distribution of a statistic without making assumptions about the underlying population distribution. It is a versatile and powerful tool for statistical inference and is widely used in various fields, including machine learning, finance, and epidemiology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1edc4d4-f20a-4bd6-aadf-0cbb09793d64",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. UseN bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711ef11-4df4-499e-bd9d-84ff17ce4cf2",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap resampling, we can follow these steps:\n",
    "\n",
    "Original Sample: Start with the original sample of 50 tree heights, with a mean of 15 meters and a standard deviation of 2 meters.\n",
    "\n",
    "Bootstrap Resampling:\n",
    "\n",
    "Generate multiple bootstrap samples by sampling with replacement from the original sample. Each bootstrap sample should have the same size as the original sample (50 in this case).\n",
    "Calculate the mean height for each bootstrap sample.\n",
    "Bootstrap Distribution:\n",
    "\n",
    "Create a distribution of bootstrap sample means obtained from the resampled datasets.\n",
    "Confidence Interval Calculation:\n",
    "\n",
    "Calculate the 95% confidence interval using the bootstrap distribution of sample means.\n",
    "The confidence interval can be obtained by finding the 2.5th and 97.5th percentiles of the bootstrap distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16432cf2-20b1-47fd-b000-81ace7d9b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Population Mean Height:\n",
      "(14.46 meters, 15.56 meters)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample statistics\n",
    "original_mean = 15  # meters\n",
    "original_std = 2  # meters\n",
    "sample_size = 50\n",
    "\n",
    "# Generate bootstrap samples\n",
    "num_bootstrap_samples = 1000\n",
    "bootstrap_means = []\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Generate bootstrap sample by sampling with replacement\n",
    "    bootstrap_sample = np.random.normal(original_mean, original_std, size=sample_size)\n",
    "    # Calculate mean height for bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval for the Population Mean Height:\")\n",
    "print(f\"({confidence_interval[0]:.2f} meters, {confidence_interval[1]:.2f} meters)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bfecb-945b-41d8-b7a0-75a09e930863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
